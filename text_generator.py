#!/bin/python
# -*- coding: utf-8 -*-

"""
A random text generator using Markov chains. On the training phase pairs of consecutive words
are parsed for following words are stored as a cache. Text is then generated by randomly
picking 3-word combinations from the cache.

based on
https://gist.github.com/agiliq/131679#file-gistfile1-py
https://github.com/codebox/markov-text

Changelog
4.2.2017
 * Added an INDEX to the database to increase lookup performance. Apparently
   the UNIQUE constarint also came with an INDEX.
2.2.2017
 * Minor sentence generation tweaks: added option to continue generating until a
   punctuation token is encoutered.
 * Added an option to provide a custom seed string to generate_markov_text
   as an alternative to choosing it randomly from the database.
 * Database rows are no longer UNIQUE so when choosing successors from the database
   probabilities are now properly considered, as they should be in a Markov process.
   Ie. if a word has several matches with identical rows, it's more likely that
   that row gets chosen.
 * Training now ignores some special characters like "@" and "http"

3.1.2017
 * Initial version
"""



import random
import json
import sys
import argparse
import codecs
import math
import glob
import os
import sqlite3 as lite

import parse_input


class MarkovGenerator():

	def __init__(self, *args, **kwargs):
		"""An initializer accepting two ways to create a new generator: either pass in a
		path to a training data file for training a new generator, or a path to an existing
		cache file when selecting one of previously trained generators.
		kwargs:
			input (string): path to a file to be used to train a generator
			cache (string): path to an existing .cache file
				Note: One of these should be present.
			depth (int): the depth of the Markov chain used to train the generator. This only
			matters when "input" is present. Determines how many of the current
				text should be matched to new data from the cache. For a true Markov chain
				the default value of 1 should be used*. High values will generate text barely
				different from the original text.
				* A true Markov chain process would also consider probabilities when determining
				  the next word from the cache, this script chooses them randomly.
		"""
		self.input = kwargs.get("input", None)
		self.cache = kwargs.get("cache", None)
		self.depth = kwargs.get("depth", 2) # ngram length, ie. number of words to store in a database row

		# Either an input file or cache file needs to be present. (though this should always be the case
		# since creating a generator object is well controlled under main)
		if self.input is None and self.cache is None:
			print "ERROR: Neither a training file, nor an existing cache file was specified."
			sys.exit()

		# If the input argument was specified, define a location for the cache under the data folder:
		# keep the basename and change the extension.
		if self.input:
			filename = os.path.basename(self.input)
			self.cache = "data/" + os.path.splitext(filename)[0] + ".cache"

		
	def ngrams(self):
		"""Processes the input text into ngrams. For n=3 generates as
		"What a lovely day" => [ [What, a, lovely], [a, lovely, day] ]
		Yield:
			the next ngram
		"""
		# Read the training data from file and split by words.
		with codecs.open(self.input, "r", "utf8") as f:
			train_data = f.read().split()
			train_data = [word for word in train_data if not any(item in word for item in ("http://", "https://", "@", "#"))] # remove urls and email addresses

		#n = self.depth
		if len(train_data) < self.depth:
			return
		
		# Yield the next batch of ngrams
		for i in range(len(train_data) - (self.depth - 1)): # the starting index should ignore the last n words
			yield train_data[i: i + self.depth]
			

	def train(self):
		"""Train a generator by storing all detected ngrams to a database with n word columns."""
		# Check if a cache file already exists.
		if os.path.isfile(self.cache):
			ans = raw_input("WARNING: There already is a cache for " + self.input + ". Are you sure you want to replace it (N/y)?\n")
			if ans != "y":
				sys.exit()

		# Check if self.input is a folder and parse its contents to temp file for training.
		temp_created = False
		if os.path.isdir(self.input):
			self.input = parse_input.parse_folder(self.input)
			temp_created = True


		con = lite.connect(self.cache)
		cur = con.cursor()

		# Generate the SQL for "CREATE TABLE ngrams (w1 TEXT NOT NULL, w2 TEXT NOT NULL, ... wn TEXT NOT NULL)",
		# column names can't be targeted for parameter substitution.
		coldefs = ["w{} TEXT NOT NULL".format(i) for i in range(1, self.depth+1)]
		sql = "CREATE TABLE ngrams (" + ", ".join(coldefs) + ")"

		print "Building a database with depth {}...".format(self.depth-1)
		with con:
			cur.execute("DROP TABLE IF EXISTS ngrams")  # remove previous data
			cur.execute(sql)

			for ngram in self.ngrams():
				try:
					subs = ", ".join( ["?" for i in range(self.depth)] )  # a string of "?, ?, ..., ?" for parameter substitution
					sql = "INSERT INTO ngrams VALUES ({})".format(subs)
					cur.execute(sql, tuple(ngram))

			# Create an index from the first n-1 columns.
			print "Creating a table index"
			index_cols = ", ".join(["w{}".format(i) for i in range(1, self.depth)])
			sql = "CREATE INDEX ngrams_index ON ngrams({})".format(index_cols)
			cur.execute(sql)

		# Delete the input file if it was a temp file created in __init__().
		if temp_created:
			os.remove(self.input)
			print "Temporary files deleted."

		
	def generate_markov_text(self, size = 25, seed = None, complete_sentence = False):
		"""Generates a string of length size by randomly selecting words from the cache database such that
		the first n-1 words of the row fetched matches the last n-1 words of the current generated text.
		Arg:
			size (int): number of words the text should contain.
			seed (string): the initial word where to start the generation from. If not set, the seed will be selected
				from the database. If no successor is found to the seed, a ValueError is thrown
			complete_sentence (boolean): whether to continue adding words past size to the sentence until a punctuation
				character or a capitalized word is encoutered.
		Return:
			the generated text
		"""
		if not os.path.isfile(self.cache):
			print "ERROR: No cache file for " + self.input + ". Run with --train to create one first."
			sys.exit()

		con = lite.connect(self.cache)
		cur = con.cursor()

		words = []
		with con:
			cur.execute("SELECT * FROM ngrams ORDER BY RANDOM() LIMIT 1")
			row = cur.fetchone()
			depth = len(row)

			# Set the initial words for the generated text: either the row fetched or any
			# input passed in through the seed parameter.
			if seed:
				split = seed.split()
				# Check if the seed is long enough:
				if len(split) < depth - 1:
					raise ValueError("Invalid seed: Need at least {} words, received \"{}\"".format(depth-1, seed))

				# Check wheter the database contains a row starting with the last depth-1 words of the seed
				row = split[-(depth-1):]  # the last depth-1 words
				cols = " AND ".join( ["w{} = ?".format(i) for i in range(1, depth)] ) # format the SQL for a "...WHERE w1 = ? AND w2 = ? ... AND w(n-1) = ?" query
				sql = "SELECT * FROM ngrams WHERE {}".format(cols)
				cur.execute(sql, row)
				data = cur.fetchone()
				if not data:
					raise ValueError("Invalid seed: database doesn't contain a row beginning with {}".format(row))

				words.extend(split)  # Set the whole seed as the initial words for the generated text,
				row = split[-depth:]  # ...but only pass the last depth words to the generation algorithm.
			else:
				words.extend(row)
			

			# Fetch rows and add new words one at a time until text length == size.
			# The first n-1 words of the next row should match to the n-1 last words
			# of the previous row.
			cols = " AND ".join( ["w{} = ?".format(i) for i in range(1, depth)] )  
			sql = "SELECT * FROM ngrams WHERE {} ORDER BY RANDOM() LIMIT 1".format(cols)
			while len(words) < size:
				cur.execute(sql, row[1:])
				row = cur.fetchone()
				words.append(row[-1])  # add the last word as the next word generated


			# Continue adding words until one that ends with a sentence ending character.
			if complete_sentence:
				last = words[-1]
				while not last.endswith((".", "!", "?", "...", ":", ";", ",", "-", u"…")):
					cur.execute(sql, tuple(row[1:]))
					row = cur.fetchone()
					next_ = row[-1]

					words.append(next_)
					last = next_
		

		# Return a properly capitalized and punctuated string.
		return MarkovGenerator.cleanup(words)


	####################
	# Helper functions #
	####################

	def get_depth(self):
		"""Determine the depth (ie. number of columns) of the cache file database."""
		con = lite.connect(self.cache)
		cur = con.cursor()

		with con:
			cur.execute("SELECT * FROM ngrams LIMIT 1")
			ncol = len(cur.fetchone())

		return ncol


	@staticmethod
	def cleanup(tokens):
		"""cleanup a sentence by capitalizing the first letter, remove conjuctions like "and" and "to"
		from the end and add a punctuation mark.
		Arg:
			tokens (list): the sentence to normalize as a list of words
		Return:
			the normalized sentence as a string
		"""
		# Capitalize the first word (calling capitalize() on the whole string would
		# decapitalize everyting else).
		tokens[0] = tokens[0].capitalize()
		text = " ".join(tokens)
		text = text.lstrip(" -*")
		
		# Replace opening parathesis with a comma and remove closing paranthesesis and
		# replace other inconvenient characters.
		replacements = [
			(" (", ","),
			("(", ""), # in case the first character of a sentence is "("
			(")", ""),
			("\"", ""),
			(u"“", ""),
			(u"”", ""),
			(u"”", ""),
			(u"…", "...")
		]
		for item in replacements:
			text = text.replace(item[0], item[1])


		text = text.rstrip(",;:- ")
		if not text.endswith((".", "!", "?", "...")):
			rand = random.random()
			if rand < 0.81:
				end = "."  # "." should have the greatest change of getting selected
			else:
				end = random.choice(("!", "?", "..."))  # choose evenly between the rest

			text += end

		return text


	@staticmethod
	def show_menu(files):
		"""Prints a list of existing .cache files and asks for user input on which
		generator to use.
		Args:
			files (list): a list of filepaths to .cache files in the data folder
		Return:
			the path to the .cache the user selected
		"""
		print "Select which existing generator to use:"
		for i, file in enumerate(files):
			print i+1, os.path.basename(file)

		# Ask for user input and create a generator.
		ans = int(raw_input("Input: "))
		path_to_cache = files[ans-1]

		return path_to_cache


	########
	# Main #
	########

if __name__ == "__main__":
	parser = argparse.ArgumentParser(description="Generates random text based on an input sample.")
	parser.add_argument("input", help="Text file containing a writing sample used to train the generator.", nargs="?")
	parser.add_argument("--train", help="Train the generator. Creates a cache file needed to generate random text.", action="store_true")
	parser.add_argument("--generate", help="Generate text with p paragraphs with about n words each.", nargs=2, type=int, metavar = ("p", "n"))
	parser.add_argument("--depth", help="How many previous words should be considered when generating new text. Valid values are 1,2 and 3.",
		nargs="?", metavar="depth", type=int, default=1, const=1, choices=[1,2,3])
	args = parser.parse_args()


	# If no input file present, list the existing trained cache files and ask for user input.
	if not args.input:
		files = glob.glob("data/*.cache")
		path_to_cache = MarkovGenerator.show_menu(files)

		# Generate a text of random length.
		gen = MarkovGenerator(cache = path_to_cache)
		n = int(random.gauss(25, 25/4.0)) # select text length from a Gaussian distribution

		print gen.generate_markov_text(n)

		# Display an interactive menu for generating more text, returning to the menu or exiting until the user exits.
		while True:
			ans = raw_input("***usage: [m]enu, [e]xit, any other key for more.***\n")
			if ans == "m":
				path_to_cache = MarkovGenerator.show_menu(files)
				gen = MarkovGenerator(cache = path_to_cache)
				print gen.generate_markov_text(n)

			elif ans in ("e", "q"):
				sys.exit()

			else:
				print gen.generate_markov_text(n)



	# Create a generator based on user input, invalid filepath causes a sys.exit() call.
	else:
		# Check user input is a valid path,
		# Note: it's enough to check for files since __init__() checks for folders anyway.
		if not (os.path.isfile(args.input) or os.path.isdir(args.input)):
			print "ERROR: No such file or folder " + args.input
			sys.exit()

		gen = MarkovGenerator(input = args.input, depth = args.depth + 1)

	if args.train:
		gen.train()
		print "Cache stored at", gen.cache

	elif args.generate:
		p = args.generate[0]
		n = args.generate[1]

		paragraphs = []
		for i in range(p):
			p_size = int(random.gauss(n, n/4.0))
			paragraphs.append(gen.generate_markov_text(p_size))

		text = "\n\n".join(paragraphs)
		print text

